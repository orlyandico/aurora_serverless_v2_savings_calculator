{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf208013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate potential savings in USD from moving your existing RDS fleet to Aurora Serverless V2\n",
    "#\n",
    "# BUGS\n",
    "# - hard-wired Aurora Serverless V2 ACU pricing\n",
    "# - assumes all On-Demand, does not factor in Reserved Instances\n",
    "# - assumes 1 ACU = 0.25 vCPU which (probably) is fine for now but may change\n",
    "# - doesn't work well for burstable instances (assumes that they are equivalent to regular instances)\n",
    "# - not tested on account with large number of DB instances (does describe_db_instances paginate?)\n",
    "# - requires that your AWS CLI is properly set up and with proper AWS access/secret keys and region\n",
    "# - only queries RDS fleet in current region (defined in AWS CLI configuration)\n",
    "#\n",
    "# NEW FEATURE\n",
    "# - calculates IOPS cost when moving from EBS storage to Aurora storage\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n",
    "# TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHOR OR COPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n",
    "# CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n",
    "# IN THE SOFTWARE.\n",
    "#\n",
    "# receiver of blame: orly.andico@gmail.com\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "client = boto3.client('rds')\n",
    "cw = boto3.client('cloudwatch')\n",
    "sess = boto3.session.Session()\n",
    "region = sess.region_name\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a87ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.describe_db_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94773da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.concat([df, pd.DataFrame(response['DBInstances']) ], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae05597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(['DBInstanceIdentifier','DBInstanceClass','Engine', 'DBInstanceStatus', 'AllocatedStorage', 'SecondaryAvailabilityZone'], axis=1)\n",
    "df2 = df2.astype({\"AllocatedStorage\": int, \"SecondaryAvailabilityZone\": str})\n",
    "\n",
    "# we need to remove DBInstanceClass = \"db.serverless\" which corresponds to serverless v2\n",
    "# serverless v1 does not show up in describe_db_instances\n",
    "df2 = df2[ df2['DBInstanceClass'] != 'db.serverless'].reset_index(drop=True)\n",
    "\n",
    "# we also need to remove any databases which aren't supported (i.e. not MySQL or PostgreSQL)\n",
    "df2 = df2[ df2['Engine'].isin(['aurora-mysql', 'mysql', 'aurora-postgresql', 'postgresql'])]\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pricing data\n",
    "# this only really works for MySQL and PostgreSQL because we hard-wire the no license required\n",
    "def get_rds_instance_hourly_price(region_name, instance_type, database_engine, deployment_option):\n",
    "\n",
    "    filters = [\n",
    "        {'Type': 'TERM_MATCH', 'Field': 'instanceType', 'Value': instance_type},\n",
    "        {'Type': 'TERM_MATCH', 'Field': 'databaseEngine', 'Value': database_engine},\n",
    "        {'Type': 'TERM_MATCH', 'Field': 'licenseModel', 'Value': 'No License required'},\n",
    "        {'Type': 'TERM_MATCH', 'Field': 'deploymentOption', 'Value': deployment_option},        \n",
    "        {'Type': 'TERM_MATCH', 'Field': 'regionCode', 'Value': region_name}\n",
    "    ]\n",
    "    \n",
    "#    print (\"DEBUG: \", filters)\n",
    "\n",
    "    pricing_client = boto3.client('pricing', region_name='us-east-1')    \n",
    "    response = pricing_client.get_products(ServiceCode='AmazonRDS', Filters=filters, MaxResults=1)\n",
    "\n",
    "    j = json.loads(response['PriceList'][0])\n",
    "    od = j['terms']['OnDemand']\n",
    "    id1 = list(od)[0]\n",
    "    id2 = list(od[id1]['priceDimensions'])[0]\n",
    "\n",
    "    price_od = od[id1]['priceDimensions'][id2]['pricePerUnit']['USD']\n",
    "\n",
    "    r = {\n",
    "        'vcpu': j['product']['attributes']['vcpu'],\n",
    "        'memory': j['product']['attributes']['memory'],\n",
    "        'pricePerUnit': price_od,\n",
    "        'instanceType': j['product']['attributes']['instanceType'],\n",
    "        'databaseEngine': j['product']['attributes']['databaseEngine'],\n",
    "        'deploymentOption': j['product']['attributes']['deploymentOption']\n",
    "    }\n",
    "    return (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb355e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the pricing for every row in the RDS instances\n",
    "\n",
    "nrows = len(df2.index)\n",
    "idx = 0\n",
    "\n",
    "# iterate over all rows in dataframe\n",
    "while (idx < nrows):\n",
    "    db = df2.iloc[idx]['Engine']\n",
    "    az = df2.iloc[idx]['SecondaryAvailabilityZone']\n",
    "    if (len(az) > 4):\n",
    "        deploymentOption = 'Multi-AZ'\n",
    "    else:\n",
    "        deploymentOption = 'Single-AZ'\n",
    "    \n",
    "    ic = df2.iloc[idx]['DBInstanceClass']\n",
    "    \n",
    "    if (db == 'mysql'):\n",
    "        databaseEngine = 'MySQL'\n",
    "    elif (db == 'aurora-mysql'):\n",
    "        databaseEngine = 'Aurora MySQL'\n",
    "    elif (db == 'postgresql'):\n",
    "        databaseEngine = 'PostgreSQL'\n",
    "    elif (db == 'aurora-postgresql'):\n",
    "        databaseEngine = 'Aurora PostgreSQL'\n",
    "    else:\n",
    "        databaseEngine = 'MySQL'\n",
    "    \n",
    "    \n",
    "    r = get_rds_instance_hourly_price(region, ic, databaseEngine, deploymentOption)\n",
    "    \n",
    "    # sanity check that we got the correct match\n",
    "    if (ic == r['instanceType'] and\n",
    "        databaseEngine == r['databaseEngine'] and\n",
    "       deploymentOption == r['deploymentOption']):\n",
    "        \n",
    "        r['memory'] = r['memory'].replace(\" GiB\", \"\") \n",
    "        \n",
    "        r['memory'] = float(r['memory'])\n",
    "        r['pricePerUnit'] = float(r['pricePerUnit'])\n",
    "        r['vcpu'] = float(r['vcpu'])\n",
    "\n",
    "        # NOTE: multi-AZ deployments have the 2 instances factored into the cost!\n",
    "        # no need to multiply by 2\n",
    "        if (deploymentOption == 'Multi-AZ'):\n",
    "            num = 1\n",
    "        else:\n",
    "            num = 1\n",
    "            \n",
    "        print(r, \"\\n\")\n",
    "        df2.loc[idx, ['pricePerUnit', 'deploymentOption', 'vcpu', 'memory', 'pricePerMonth']] = [r['pricePerUnit'], r['deploymentOption'], r['vcpu'], r['memory'], r['pricePerUnit'] * 730 * num ]\n",
    "                                                                                \n",
    "    idx = idx + 1\n",
    "                                                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c40caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can only fetch 1440 data points from Cloudwatch, so over a 2-week period (20160 minutes)\n",
    "# our sampling interval is 14 minutes; also note we are fetching the *MAXIMUM* over each sampling period\n",
    "# however, let's use a less aggressive 1-hour sampling interval\n",
    "\n",
    "# note ReadIOPS/WriteIOPS are for RDS EBS engines (and Aurora PostgreSQL)\n",
    "# VolumeReadIOPS/VolumeWriteIOPS are for Aurora MySQL and PostgreSQL *but only on cluster level*\n",
    "\n",
    "# we don't really need IOPS for Aurora because moving from Aurora Provisioned to Aurora Serverless won't change the IOPS cost\n",
    "# but moving from RDS EBS engines to Aurora storage will\n",
    "\n",
    "dfutil = pd.DataFrame()\n",
    "dfiops_read = pd.DataFrame()\n",
    "dfiops_write = pd.DataFrame()\n",
    "\n",
    "for dbid in df2['DBInstanceIdentifier']:\n",
    "    \n",
    "    # this filter returns a series, but the series only contains one item\n",
    "    engine = df2[ (df2['DBInstanceIdentifier'] == dbid) ].Engine.item()\n",
    "\n",
    "    # CPUUtilization\n",
    "    stats = cw.get_metric_statistics(\n",
    "        Namespace='AWS/RDS',\n",
    "        Dimensions=[\n",
    "            {\n",
    "                'Name': 'DBInstanceIdentifier',\n",
    "                'Value': dbid\n",
    "            }\n",
    "        ],\n",
    "        MetricName='CPUUtilization',\n",
    "        StartTime=datetime.now() - timedelta(days=14),\n",
    "        EndTime=datetime.now(),\n",
    "        Period=3600,\n",
    "        Statistics=[ 'Maximum' ])\n",
    "    df3 = pd.DataFrame(stats['Datapoints'])\n",
    "    df3['DBInstanceIdentifier'] = dbid\n",
    "\n",
    "    dfutil = pd.concat([dfutil, df3], ignore_index=True)\n",
    "\n",
    "    # skip IOPS for aurora engines\n",
    "    if ( (engine != 'aurora-mysql') and (engine != 'aurora-postgresql') ):\n",
    "        metricname=\"ReadIOPS\"\n",
    "\n",
    "        # ReadIOPS\n",
    "        stats = cw.get_metric_statistics(\n",
    "            Namespace='AWS/RDS',\n",
    "            Dimensions=[\n",
    "                {\n",
    "                    'Name': 'DBInstanceIdentifier',\n",
    "                    'Value': dbid\n",
    "                }\n",
    "            ],\n",
    "            MetricName=metricname,\n",
    "            StartTime=datetime.now() - timedelta(days=14),\n",
    "            EndTime=datetime.now(),\n",
    "            Period=3600,\n",
    "            Statistics=[ 'Maximum' ])\n",
    "        df3 = pd.DataFrame(stats['Datapoints'])\n",
    "        df3['DBInstanceIdentifier'] = dbid\n",
    "\n",
    "        dfiops_read = pd.concat([dfiops_read, df3], ignore_index=True)\n",
    "\n",
    "    if ((engine != 'aurora-mysql') and (engine != 'aurora-postgresql')):\n",
    "        metricname=\"WriteIOPS\"\n",
    "\n",
    "        # WriteIOPS\n",
    "        stats = cw.get_metric_statistics(\n",
    "            Namespace='AWS/RDS',\n",
    "            Dimensions=[\n",
    "                {\n",
    "                    'Name': 'DBInstanceIdentifier',\n",
    "                    'Value': dbid\n",
    "                }\n",
    "            ],\n",
    "            MetricName=metricname,\n",
    "            StartTime=datetime.now() - timedelta(days=14),\n",
    "            EndTime=datetime.now(),\n",
    "            Period=3600,\n",
    "            Statistics=[ 'Maximum' ])\n",
    "        df3 = pd.DataFrame(stats['Datapoints'])\n",
    "        df3['DBInstanceIdentifier'] = dbid\n",
    "\n",
    "        dfiops_write = pd.concat([dfiops_write, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f901bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each DBInstanceIdentifier, get the average and maximum CPU utilization\n",
    "df_agg = dfutil.groupby(\"DBInstanceIdentifier\").Maximum.agg([\"mean\", \"std\", \"max\", \"count\"]).reset_index()\n",
    "\n",
    "df_agg.columns = [ \"DBInstanceIdentifier\", \"meanCPU\", \"stdCPU\", \"maxCPU\", \"countCPU\" ]\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66506e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each DBInstanceIdentifier, get the average and maximum Read IOPS\n",
    "df_agg_r = dfiops_read.groupby(\"DBInstanceIdentifier\").Maximum.agg([\"mean\", \"std\", \"max\", \"count\"]).reset_index()\n",
    "\n",
    "df_agg_r.columns = [ \"DBInstanceIdentifier\", \"meanReadIOPS\", \"stdReadIOPS\", \"maxReadIOPS\", \"countReadIOPS\" ]\n",
    "df_agg_r\n",
    "\n",
    "\n",
    "# for each DBInstanceIdentifier, get the average and maximum Write IOPS\n",
    "df_agg_w = dfiops_write.groupby(\"DBInstanceIdentifier\").Maximum.agg([\"mean\", \"std\", \"max\", \"count\"]).reset_index()\n",
    "df_agg_w.columns = [ \"DBInstanceIdentifier\", \"meanWriteIOPS\", \"stdWriteIOPS\", \"maxWriteIOPS\", \"countWriteIOPS\" ]\n",
    "\n",
    "df_iops = pd.merge(df_agg_r, df_agg_w, left_on=\"DBInstanceIdentifier\", right_on=\"DBInstanceIdentifier\", how=\"left\")\n",
    "\n",
    "df_iops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa77cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c1 = pd.merge(df2, df_agg, left_on=\"DBInstanceIdentifier\", right_on=\"DBInstanceIdentifier\", how=\"left\")\n",
    "\n",
    "df_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16accbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.merge(df_c1, df_iops, left_on=\"DBInstanceIdentifier\", right_on=\"DBInstanceIdentifier\", how=\"left\")\n",
    "\n",
    "df_combined.meanReadIOPS = df_combined.meanReadIOPS.fillna(0)\n",
    "df_combined.stdReadIOPS = df_combined.stdReadIOPS.fillna(0)\n",
    "df_combined.maxReadIOPS = df_combined.maxReadIOPS.fillna(0)\n",
    "df_combined.countReadIOPS = df_combined.countReadIOPS.fillna(0)\n",
    "\n",
    "df_combined.meanWriteIOPS = df_combined.meanWriteIOPS.fillna(0)\n",
    "df_combined.stdWriteIOPS = df_combined.stdWriteIOPS.fillna(0)\n",
    "df_combined.maxWriteIOPS = df_combined.maxWriteIOPS.fillna(0)\n",
    "df_combined.countWriteIOPS = df_combined.countWriteIOPS.fillna(0)\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 GB RAM = 1 ACU, and very roughly, 1 ACU = 0.25 vCPU\n",
    "# we currently do not recommend < 2 ACU for various reasons.. (although 0.5 ACU is the stated minimum)\n",
    "df_combined['acu_usage'] = df_combined['meanCPU'] * df_combined['vcpu'] * 4 / 50\n",
    "df_combined['acu_usage'] = df_combined['acu_usage'].apply(np.floor) + 0.5\n",
    "\n",
    "# IOPS is read+write; add 20% buffer\n",
    "df_combined['IOPS'] = (df_combined[\"meanReadIOPS\"] + df_combined[\"meanWriteIOPS\"]) * 1.2\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIXME: haven't figured out how to extract ACU pricing from the Pricing API\n",
    "### hard-wiring for now, currently APG/AMS Serverless V2 ACU pricing is identical\n",
    "### also.. no GovCloud\n",
    "### https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html\n",
    "\n",
    "df_pricing = pd.DataFrame([\n",
    "    ('us-east-1', 0.12, 0.20), # N. Virginia\n",
    "    ('us-east-2', 0.12, 0.20), # Ohio\n",
    "    ('us-west-1', 0.16, 0.22), # N. California\n",
    "    ('us-west-2', 0.12, 0.20),  # Oregon\n",
    "    ('af-south-1', 0.16, 0.26),  # Cape Town\n",
    "    ('ap-east-1', 0.22, 0.24), # HK\n",
    "    ('ap-south-2', 0.18, 0.22), # Hyderabad\n",
    "    ('ap-southeast-3', 0.22, 0.22), # Jakarta\n",
    "    ('ap-southeast-4', 0.20, 0.22), # Melbourne\n",
    "    ('ap-south-1', 0.18, 0.22), # Mumbai\n",
    "    ('ap-northeast-3', 0.20, 0.24), # Osaka\n",
    "    ('ap-northeast-2', 0.20, 0.24), # Seoul\n",
    "    ('ap-southeast-1', 0.20, 0.22), # Singapore\n",
    "    ('ap-southeast-2', 0.20, 0.22), # Sydney\n",
    "    ('ap-northeast-1', 0.20, 0.24), # Tokyo\n",
    "    ('ca-central-1', 0.14, 0.22), # Canada\n",
    "    ('eu-central-1', 0.14, 0.22), # Frankfurt\n",
    "    ('eu-west-1', 0.14, 0.22), # Ireland\n",
    "    ('eu-west-2', 0.14, 0.20), # London\n",
    "    ('eu-south-1', 0.14, 0.23), # Milan\n",
    "    ('eu-west-3', 0.14, 0.22), # Paris\n",
    "    ('eu-south-1', 0.14, 0.22), # Spain\n",
    "    ('eu-north-1', 0.14, 0.21), # Stockholm\n",
    "    ('eu-central-2', 0.14, 0.242), # Zurich\n",
    "    ('me-south-1', 0.15, 0.242), # Bahrain\n",
    "    ('me-central-1', -1, 0.242), # UAE\n",
    "    ('sa-east-1', 0.25, 0.28) # Sao Paolo\n",
    "], columns=['regionCode', 'pricePerAcu', 'perPerMIOPS'])\n",
    "\n",
    "df_pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent this from bombing out unceremoniously if the current region is not one where ServerlessV2 is available\n",
    "# (via the above hard-wired pricing list)\n",
    "try:\n",
    "    ppa = df_pricing[ df_pricing['regionCode'] == region ].values[0][1]\n",
    "    ppiops = df_pricing[ df_pricing['regionCode'] == region ].values[0][2]\n",
    "except KeyError:\n",
    "    # large value to bloat the cost\n",
    "    ppa = 999999\n",
    "    ppiops = 999999\n",
    "    \n",
    "#df_combined['acuPricePerMonth'] = df_combined['acu_usage'] * ppa * 730\n",
    "\n",
    "df_combined['acuPricePerMonth'] = np.where(df_combined['deploymentOption'] == 'Single-AZ',\n",
    "                                           df_combined['acu_usage'] * ppa * 730,\n",
    "                                          df_combined['acu_usage'] * ppa * 730 * 2)\n",
    "\n",
    "# 730 hours/month x 3600 seconds/hour = 2628000 seconds\n",
    "df_combined['miopsPricePerMonth'] = (df_combined['IOPS'] * 2628000 / 1000000) * ppiops\n",
    " \n",
    "df_combined['aurora_serverless_PricePerMonth'] = df_combined['acuPricePerMonth'] + df_combined['miopsPricePerMonth']\n",
    "df_combined['potentialSavings'] = df_combined['pricePerMonth'] - df_combined['aurora_serverless_PricePerMonth']\n",
    "\n",
    "pid = os.getpid()\n",
    "outputfile = \"aurora_serverless_tco_%06d.csv\" % pid\n",
    "df_combined.to_csv(outputfile, index=False)\n",
    "\n",
    "print(\"Wrote output file %s\" % outputfile)\n",
    "df_combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
